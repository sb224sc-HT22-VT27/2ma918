{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12abadf0",
   "metadata": {},
   "source": [
    "# Exercise 2: Unconstrained Optimization I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f7b24",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "056b02f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa59f8d",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f44ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"Function to minimize: f(x, y) = (x+1)^2 - xy + 3(y-5)^2\"\"\"\n",
    "    return (x[0] + 1) ** 2 - x[0] * x[1] + 3 * (x[1] - 5) ** 2\n",
    "\n",
    "\n",
    "def grad_f(x):\n",
    "    \"\"\"Gradient of f\"\"\"\n",
    "    dx = 2 * (x[0] + 1) - x[1]\n",
    "    dy = -x[0] + 6 * (x[1] - 5)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "\n",
    "def hessian_f(x):\n",
    "    \"\"\"Hessian of f\"\"\"\n",
    "    return np.array([[2, -1], [-1, 6]])\n",
    "\n",
    "\n",
    "def hessian_inv_f(x):\n",
    "    \"\"\"Inverse Hessian of f (constant for this quadratic)\"\"\"\n",
    "    H = hessian_f(x)\n",
    "    return np.linalg.inv(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b154f35",
   "metadata": {},
   "source": [
    "## Analytical Solution (grad_f = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f39ef4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_analytical():\n",
    "    \"\"\"Find optimal point by solving grad_f = 0\n",
    "    grad_f = [2(x+1) - y, -x + 6(y-5)] = [0, 0]\n",
    "        2x + 2 - y = 0  =>  y = 2x + 2\n",
    "        -x + 6y - 30 = 0  =>  6y = x + 30\n",
    "    Substitute: 6(2x + 2) = x + 30\n",
    "        12x + 12 = x + 30\n",
    "        11x = 18\n",
    "        x = 18/11\n",
    "    \"\"\"\n",
    "\n",
    "    x_opt = 18 / 11\n",
    "    y_opt = 2 * x_opt + 2\n",
    "\n",
    "    optimal_point = np.array([x_opt, y_opt])\n",
    "    optimal_value = f(optimal_point)\n",
    "\n",
    "    print(\"Analytical Solution:\")\n",
    "    print(f\"\\tOptimal point: ({x_opt:.6f}, {y_opt:.6f})\")\n",
    "    print(f\"\\tOptimal value: {optimal_value:.6f}\")\n",
    "    print(f\"\\tGradient at optimal: {grad_f(optimal_point)}\")\n",
    "    print(f\"\\tHessian:\\n{hessian_f(optimal_point)}\")\n",
    "\n",
    "    # Check if Hessian is positive definite\n",
    "    eigenvalues = np.linalg.eigvals(hessian_f(optimal_point))\n",
    "    print(f\"\\tHessian eigenvalues: {eigenvalues}\")\n",
    "    print(f\"\\tPositive definite: {np.all(eigenvalues > 0)}\")\n",
    "\n",
    "    return optimal_point, optimal_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2609fb51",
   "metadata": {},
   "source": [
    "## Line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "186c3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def armijo_line_search(x_k, grad_k, f_func):\n",
    "    \"\"\"Armijo's method for step length selection\"\"\"\n",
    "    alpha = 1.0\n",
    "    grad_norm_sq = grad_k @ grad_k\n",
    "\n",
    "    def w(t_val):\n",
    "        return x_k - t_val * grad_k\n",
    "\n",
    "    f_xk = f_func(x_k)\n",
    "\n",
    "    # Check initial condition\n",
    "    if f_func(w(alpha)) <= f_xk - 0.2 * alpha * grad_norm_sq:\n",
    "        # Need to increase alpha\n",
    "        while f_func(w(alpha)) <= f_xk - 0.2 * alpha * grad_norm_sq:\n",
    "            t_prev = alpha\n",
    "            alpha = 2 * alpha\n",
    "            if alpha > 1e6:\n",
    "                return t_prev\n",
    "        return t_prev\n",
    "    else:\n",
    "        # Need to decrease alpha\n",
    "        while f_func(w(alpha)) > f_xk - 0.2 * alpha * grad_norm_sq:\n",
    "            alpha = alpha / 2\n",
    "            if alpha < 1e-10:\n",
    "                return alpha\n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb704b8f",
   "metadata": {},
   "source": [
    "## Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6908f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent(x0, tol=1e-3, max_iter=10000):\n",
    "    \"\"\"Steepest descent method with Armijo line search\"\"\"\n",
    "    x_k = x0.copy()\n",
    "    iterations = 0\n",
    "    points = [x_k.copy()]\n",
    "\n",
    "    while True:\n",
    "        grad_k = grad_f(x_k)\n",
    "        grad_norm = np.linalg.norm(grad_k)\n",
    "\n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "\n",
    "        if iterations >= max_iter:\n",
    "            print(f\"Warning: Maximum iterations ({max_iter}) reached\")\n",
    "            break\n",
    "\n",
    "        t_k = armijo_line_search(x_k, grad_k, f)\n",
    "\n",
    "        # Update\n",
    "        x_k = x_k - t_k * grad_k\n",
    "        points.append(x_k.copy())\n",
    "        iterations += 1\n",
    "\n",
    "    return x_k, iterations, points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ddf8a",
   "metadata": {},
   "source": [
    "## Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "118e98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtons_method(x0, tol=1e-3, max_iter=100):\n",
    "    \"\"\"Newton's method for optimization\"\"\"\n",
    "    x_k = x0.copy()\n",
    "    iterations = 0\n",
    "    points = [x_k.copy()]\n",
    "\n",
    "    while True:\n",
    "        grad_k = grad_f(x_k)\n",
    "        grad_norm = np.linalg.norm(grad_k)\n",
    "\n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "\n",
    "        if iterations >= max_iter:\n",
    "            print(f\"Warning: Maximum iterations ({max_iter}) reached\")\n",
    "            break\n",
    "\n",
    "        # Newton update\n",
    "        H_inv = hessian_inv_f(x_k)\n",
    "        x_k = x_k - H_inv @ grad_k\n",
    "        points.append(x_k.copy())\n",
    "        iterations += 1\n",
    "\n",
    "    return x_k, iterations, points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28435fff",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c07313c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical Solution:\n",
      "\tOptimal point: (1.636364, 5.272727)\n",
      "\tOptimal value: -1.454545\n",
      "\tGradient at optimal: [0.00000000e+00 3.77475828e-15]\n",
      "\tHessian:\n",
      "[[ 2 -1]\n",
      " [-1  6]]\n",
      "\tHessian eigenvalues: [1.76393202 6.23606798]\n",
      "\tPositive definite: True\n",
      "\n",
      "Steepest Descent Method:\n",
      "\tStarting point: (1.000000, 1.000000)\n",
      "\tFinal point: (1.636346, 5.272606)\n",
      "\tNumber of iterations: 18\n",
      "\tFinal function value: -1.454545\n",
      "\tOptimal function value: -1.454545\n",
      "\tAbsolute error: 4.237244e-08\n",
      "\tFinal gradient norm: 7.156404e-04\n",
      "\n",
      "Newton's Method:\n",
      "\tStarting point: (1.000000, 1.000000)\n",
      "\tFinal point: (1.636364, 5.272727)\n",
      "\tNumber of iterations: 1\n",
      "\tFinal function value: -1.454545\n",
      "\tOptimal function value: -1.454545\n",
      "\tAbsolute error: 0.000000e+00\n",
      "\tFinal gradient norm: 3.552714e-15\n"
     ]
    }
   ],
   "source": [
    "# Analysis\n",
    "x_optimal, f_optimal = find_optimal_analytical()\n",
    "\n",
    "\n",
    "# Steepest Descent\n",
    "x0 = np.array([1.0, 1.0])\n",
    "print(\"\\nSteepest Descent Method:\")\n",
    "x_sd, iter_sd, points_sd = steepest_descent(x0)\n",
    "print(f\"\\tStarting point: ({x0[0]:.6f}, {x0[1]:.6f})\")\n",
    "print(f\"\\tFinal point: ({x_sd[0]:.6f}, {x_sd[1]:.6f})\")\n",
    "print(f\"\\tNumber of iterations: {iter_sd}\")\n",
    "print(f\"\\tFinal function value: {f(x_sd):.6f}\")\n",
    "print(f\"\\tOptimal function value: {f_optimal:.6f}\")\n",
    "print(f\"\\tAbsolute error: {abs(f(x_sd) - f_optimal):.6e}\")\n",
    "print(f\"\\tFinal gradient norm: {np.linalg.norm(grad_f(x_sd)):.6e}\")\n",
    "\n",
    "\n",
    "# Newton's Method\n",
    "print(\"\\nNewton's Method:\")\n",
    "x_nm, iter_nm, points_nm = newtons_method(x0)\n",
    "print(f\"\\tStarting point: ({x0[0]:.6f}, {x0[1]:.6f})\")\n",
    "print(f\"\\tFinal point: ({x_nm[0]:.6f}, {x_nm[1]:.6f})\")\n",
    "print(f\"\\tNumber of iterations: {iter_nm}\")\n",
    "print(f\"\\tFinal function value: {f(x_nm):.6f}\")\n",
    "print(f\"\\tOptimal function value: {f_optimal:.6f}\")\n",
    "print(f\"\\tAbsolute error: {abs(f(x_nm) - f_optimal):.6e}\")\n",
    "print(f\"\\tFinal gradient norm: {np.linalg.norm(grad_f(x_nm)):.6e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
