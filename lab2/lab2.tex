\documentclass{article}
\usepackage{graphicx}
\usepackage[colorlinks=true,
  linkcolor=black,
  urlcolor=blue,
citecolor=black]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{cite}
\usepackage[nottoc]{tocbibind}
\usepackage[english]{babel}

\tolerance=1413
\hfuzz=1.5pt

\title{2MA918 Laboration II}
\author{Samuel Berg (sb224sc)}
\date{December 2025}

\begin{document}

\maketitle

\tableofcontents

\newpage
\section{Exercise 1: Minimum Cost Flow Problems}

This exercise involves solving minimum cost network flow problems for a logistics company in the Kronoberg region.

\subsection{Part 1: Initial Network Flow Problem}

The company owns two production facilities in Älmhult (supply: 2500 units) and Markaryd (supply: 1000 units), with two storage terminals in Liatorp and Osby. They need to transport units to retail stores in Ljungby (demand: 1000), Alvesta (demand: 500), and Växjö (demand: 2000).

\textbf{Formulation:} This is formulated as a linear programming problem where we minimize the total transportation cost subject to:
\begin{itemize}
  \item Flow conservation constraints at each node
  \item Capacity constraints on each edge
  \item Supply and demand constraints
\end{itemize}

\textbf{Solution:} Using scipy's linprog with the HiGHS method:
\begin{itemize}
  \item \textbf{Minimum cost:} 250,000.00
  \item \textbf{Optimal flow:}
    \begin{itemize}
      \item Älmhult $\rightarrow$ Liatorp: 1000 units (cost: 18)
      \item Älmhult $\rightarrow$ Osby: 500 units (cost: 24)
      \item Älmhult $\rightarrow$ Vaxjo: 1000 units (cost: 62)
      \item Osby $\rightarrow$ Markaryd: 500 units (cost: 29)
      \item Liatorp $\rightarrow$ Vaxjo: 1000 units (cost: 46)
      \item Markaryd $\rightarrow$ Ljungby: 1500 units (cost: 51)
      \item Ljungby $\rightarrow$ Alvesta: 500 units (cost: 42)
    \end{itemize}
\end{itemize}

\subsection{Part 2: New Production Facility Analysis}

The company is considering adding a new production facility in either Värnamo or Vislanda.

\subsubsection{Värnamo Option}
With Värnamo producing 750 units and adjusted production at Älmhult (2000) and Markaryd (750):
\begin{itemize}
  \item \textbf{Minimum cost:} 200,000.00
\end{itemize}

\subsubsection{Vislanda Option}
With Vislanda producing 380 units and adjusted production at Älmhult (2120) and Markaryd (1000):
\begin{itemize}
  \item \textbf{Minimum cost:} 200,220.00
\end{itemize}

\textbf{Recommendation:} \textbf{Värnamo should be chosen} as it provides the lowest cost (200,000.00), saving 220.00 compared to Vislanda and reducing the total cost by 50,000.00 from the initial network.

\newpage
\section{Exercise 2: Unconstrained Optimization I}

This exercise implements steepest descent and Newton's method to minimize $f(x, y) = {(x+1)}^2 - xy + 3{(y-5)}^2$.

\subsection{Analytical Solution}

The optimal point is found by solving $\nabla f = 0$:
\begin{align*}
  \frac{\partial f}{\partial x} &= 2(x+1) - y = 0 \\
  \frac{\partial f}{\partial y} &= -x + 6(y-5) = 0
\end{align*}

Solving this system yields:
\begin{itemize}
  \item \textbf{Optimal point:} $x^* = (1.636364, 5.272727)$
  \item \textbf{Optimal value:} $f(x^*) = -1.454545$
\end{itemize}

The Hessian matrix is:
\[
  H_f =
  \begin{bmatrix} 2 & -1 \\ -1 & 6
  \end{bmatrix}
\]
with eigenvalues $\lambda_1 = 1.764$ and $\lambda_2 = 6.236$, confirming positive definiteness and that $x^*$ is a minimum.

\subsection{Steepest Descent Method}

Starting from $x_0 = (1, 1)$ with Armijo line search:
\begin{itemize}
  \item \textbf{Number of iterations:} 18
  \item \textbf{Final point:} $(1.636346, 5.272606)$
  \item \textbf{Final function value:} $-1.454545$
  \item \textbf{Absolute error:} $4.24 \times 10^{-8}$
  \item \textbf{Final gradient norm:} $7.16 \times 10^{-4}$
\end{itemize}

\subsection{Newton's Method}

Starting from the same point $x_0 = (1, 1)$:
\begin{itemize}
  \item \textbf{Number of iterations:} 1
  \item \textbf{Final point:} $(1.636364, 5.272727)$
  \item \textbf{Final function value:} $-1.454545$
  \item \textbf{Absolute error:} $0.0$
  \item \textbf{Final gradient norm:} $3.55 \times 10^{-15}$
\end{itemize}

\subsection{Analysis}

Newton's method is significantly more efficient for this quadratic problem, converging in a single iteration. This is because Newton's method uses second-order information (the Hessian) and achieves exact convergence for quadratic functions in one step when the Hessian is constant and positive definite. Steepest descent, using only first-order information, requires 18 iterations to reach similar accuracy.

\newpage
\section{Exercise 3: Unconstrained Optimization II}

This exercise optimizes the Rosenbrock function $f(x, y) = {(a-x)}^2 + b{(y-x^2)}^2$ with $a=1$ and $b=100$.

\subsection{Contour Plots}

The Rosenbrock function is known as a challenging optimization problem due to its narrow, curved valley. Figure~\ref{fig:rosenbrock_plots} shows both the standard and logarithmic contour plots.

\begin{figure}[!ht]
  \centering
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{img/rosenbrock_contour.png}
    \caption{Standard contour plot}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{img/rosenbrock_log.png}
    \caption{Logarithmic contour plot}
  \end{subfigure}
  \caption{Contour plots of the Rosenbrock function}\label{fig:rosenbrock_plots}
\end{figure}

\subsection{Steepest Descent Method}

Starting from $x_0 = (-1, 1.5)$ with tolerance $\|\nabla f\| < 10^{-4}$:
\begin{itemize}
  \item \textbf{Number of iterations:} 2664
  \item \textbf{Final point:} $(0.999891, 0.999781)$
  \item \textbf{Final function value:} $1.19 \times 10^{-8}$
  \item \textbf{Trajectory length:} 3.662594
  \item \textbf{Final gradient norm:} $9.79 \times 10^{-5}$
\end{itemize}

The trajectory is shown in Figure~\ref{fig:rosenbrock_sd}. The method struggles with the narrow valley, requiring many iterations to converge.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.6\textwidth]{img/rosenbrock_steepest_descent.png}
  \caption{Steepest Descent trajectory on Rosenbrock function}\label{fig:rosenbrock_sd}
\end{figure}

\subsection{Newton's Method}

Starting from the same point $x_0 = (-1, 1.5)$:
\begin{itemize}
  \item \textbf{Number of iterations:} 22
  \item \textbf{Final point:} $(1.000000, 1.000000)$
  \item \textbf{Final function value:} $1.56 \times 10^{-18}$
  \item \textbf{Trajectory length:} 3.714547
  \item \textbf{Final gradient norm:} $3.63 \times 10^{-8}$
\end{itemize}

The trajectory is shown in Figure~\ref{fig:rosenbrock_nm}. Newton's method converges much faster (22 vs 2664 iterations), though the trajectory length is similar.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.6\textwidth]{img/rosenbrock_newton.png}
  \caption{Newton's Method trajectory on Rosenbrock function}\label{fig:rosenbrock_nm}
\end{figure}

\subsection{Scipy Optimization}

Using scipy's minimize with BFGS method:
\begin{itemize}
  \item \textbf{Number of iterations:} 35
  \item \textbf{Final point:} $(1.000000, 1.000000)$
  \item \textbf{Final function value:} $1.08 \times 10^{-17}$
  \item \textbf{Final gradient norm:} $6.75 \times 10^{-8}$
\end{itemize}

\subsection{Comparison}

The results clearly demonstrate the superiority of second-order methods for non-convex optimization:
\begin{itemize}
  \item Steepest descent: 2664 iterations, very slow convergence
  \item Newton's method: 22 iterations, fast convergence with Hessian information
  \item BFGS (quasi-Newton): 35 iterations, good balance between efficiency and computational cost
\end{itemize}

For the Rosenbrock function, methods using curvature information (Newton, BFGS) are 100x more efficient than steepest descent.

\newpage
\section{Exercise 4: Constrained Optimization}

This exercise solves the constrained optimization problem:
\begin{align*}
  \min \quad & f(x, y) = {(2-x-y)}^2 + y^4 \\
  \text{s.t.} \quad & x^2 + y^2 - 4 \leq 0 \\
  & 4x + 5y - 25 \leq 0
\end{align*}

using the penalty function method with $\mu_0 = 0.1$ and $\beta = 10$.

\subsection{Initial Feasible Point}

A feasible starting point must satisfy both constraints. The origin $x_0 = (0, 0)$ is chosen:
\begin{itemize}
  \item $g_1(x_0) = 0 + 0 - 4 = -4 \leq 0$ (satisfied)
  \item $g_2(x_0) = 0 + 0 - 25 = -25 \leq 0$ (satisfied)
  \item $f(x_0) = {(2-0-0)}^2 + 0^4 = 4$
\end{itemize}

\subsection{Penalty Function Method}

The penalty function is defined as:
\[
  \alpha(x) = {\max(0, g_1(x))}^2 + {\max(0, g_2(x))}^2
\]

The augmented objective function is:
\[
  F(x, \mu) = f(x) + \mu \cdot \alpha(x)
\]

We solve successive unconstrained problems, increasing $\mu$ by factor $\beta = 10$ until $\alpha(x) < 10^{-4}$.

\subsection{Results}

The method converged in \textbf{1 iteration}:

\begin{center}
  \begin{tabular}{ccccc}
    \toprule
    Iteration & $\mu$ & $x$ & $f(x)$ & $\alpha(x)$ \\
    \midrule
    0 & 0.10 & $(1.9934, 0.0066)$ & 0.000000 & $0.00 \times 10^{0}$ \\
    \bottomrule
  \end{tabular}
\end{center}

\subsection{Optimal Solution}

\begin{itemize}
  \item \textbf{Optimal point:} $x^* = (1.9934, 0.0066)$
  \item \textbf{Optimal function value:} $f(x^*) = 0.000000$
  \item \textbf{Constraint values:}
    \begin{itemize}
      \item $g_1(x^*) = -0.0264 \leq 0$ (satisfied)
      \item $g_2(x^*) = -16.9934 \leq 0$ (satisfied)
    \end{itemize}
  \item \textbf{Gradient norm:} $\|\nabla f(x^*)\| = 7.97 \times 10^{-6}$
  \item \textbf{Penalty function:} $\alpha(x^*) = 0.0$
\end{itemize}

\subsection{Analysis}

The problem converged remarkably fast because the unconstrained minimum of $f(x, y) = {(2-x-y)}^2 + y^4$ is very close to $(2, 0)$, which satisfies both constraints. The optimal solution $(1.9934, 0.0066)$ is near this point and remains well within the feasible region. The penalty function value is effectively zero, indicating a feasible solution with no constraint violations.

% Refrences
% \newpage
% \bibliographystyle{ieeetr}
% \bibliography{ref}

% Appendix
% \newpage
% \pagenumbering{Alph}
% \setcounter{page}{1}

\end{document}
